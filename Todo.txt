Todo List:

1. Integrate T5 Model for Symptom Extraction (Top Priority)
    - Replace OpenAI API with HuggingFace T5 (local, offline model)

2. Refactor CLI to Support Model Choice (RF, LR, MLP) (High Priority)
    - Allow user to select which model to use at runtime

3. Persist All Models (High Priority)
    - Save Logistic Regression (lr_model.pkl) and MLP (mlp_model.pth)

4. Implement PyTorch Model Loader for CLI (High Priority)
    - Enable loading/switching to PyTorch MLP during CLI predictions

5. Expand Feature Selection (Medium Priority)
    - 	Add Mutual Info and RFE; create a merged ranking

6. Improve Model Training (Medium Priority)
    - Add k-fold cross-validation; tune hyperparameters

7. Enhance Evaluation (Low Priority)
    - Add confusion matrices, ROC curves, top-3 accuracy, per-class analysis

8. Finalize Presentation Visuals (High Priority)
    - Add architecture diagrams, feature plots, model comparisons, CLI flow

9. Polish Final Presentation (High Priority)
    - Polish "Since Last Time" slide, walkthrough, and CLI demo

10. Create Limitations & Future Work Section
    - Address 100% accuracy realism, dataset cleanliness, future dataset needs




///////// Limitations and Future Work Section Details //////////
A. Current Limitations:

    Dataset is clean, perfectly labeled, and symptom–disease relationships are non-overlapping.

    Models achieve 100% accuracy because:

        Each disease has a nearly unique symptom fingerprint.

        No noise, incomplete data, or overlapping disease classes are present.

    Thus, models are memorizing clean symptom patterns, not generalizing across messy, real-world ambiguity.

B. Why This Isn't Overfitting:

    We used a proper 80/20 train-test split.

    Models never saw the test set during training.

    100% accuracy on unseen test data shows that the data is just that separable, not that the models are overfitting.

    No evidence of overfitting like:

        High training accuracy but low test accuracy

        Performance drop on unseen data

    Instead, both train and test sets have identical structure, so models can perfectly map symptoms to diseases.

C. Future Work:

    Test pipeline on noisier, more realistic medical datasets (symptoms missing, misreported, overlapping diseases)

    Add synthetic noise to simulate real-world uncertainty (random missing symptoms, typos)

    Extend the pipeline to handle multi-label cases (patients with multiple conditions)

    Build confidence/probability estimates (not just top-1 prediction)

    Transition CLI to a web or mobile app for broader usability

D. Summary:
    Our models achieve 100% accuracy because of the dataset’s ideal structure — clean labels and minimal class overlap.
    This doesn’t mean our models are overfitting; it shows that the problem space in our dataset is perfectly separable.
    Future work will focus on generalizing this pipeline to messier, real-world clinical data.